{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Intake-Postgres Plugin: Joins Demo\n",
                "\n",
                "The following notebook demonstrates \"join\" functionality using the _Intake-Postgres_ plugin. Its purpose is to showcase a variety of scenarios in which an _Intake_ user may want to query their PostgreSQL-based relational datasets.\n",
                "\n",
                "Joins are to be executed within the following scenarios:\n",
                "\n",
                "- One database, two tables\n",
                "- Two databases, several tables\n",
                "\n",
                "\n",
                "## Setup\n",
                "1. Download the PostgreSQL/PostGIS Docker images. With [Docker installed](https://www.docker.com/community-edition), execute:\n",
                "    ```\n",
                "    for db_inst in $(seq 0 4); do\n",
                "        docker run -d -p $(expr 5432 + $db_inst):5432 --name intake-postgres-$db_inst mdillon/postgis:9.6-alpine;\n",
                "    done\n",
                "    ```\n",
                "    All subsequent `docker run` commands will start containers from this image.\n",
                "\n",
                "1. In the same conda environment as this notebook, install `pandas`, `sqlalchemy`, `psycopg2`, `shapely`, and (optionally) `postgresql`:\n",
                "    ```\n",
                "    conda install pandas sqlalchemy psycopg2 shapely postgresql\n",
                "    ```\n",
                "    The `postgresql` package is only for the command-line client library, so that we can verify that results were written to the database (externally from our programs).\n",
                "\n",
                "1. Finally, install the _intake-postgres_ plugin:\n",
                "    ```\n",
                "    conda install -c intake intake-postgres\n",
                "    ```\n",
                "\n",
                "\n",
                "## Loading the data\n",
                "\n",
                "Because _Intake_ only supports reading the data, we need to insert the data into our databases by another means. The general approach below relies on partitioning a pre-downloaded CSV file and inserting its partitions into each table. This can be thought of as a rudimentary form of application-level \"sharding\".\n",
                "\n",
                "The code (below) begins by importing the necessary modules:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import print_function, absolute_import\n",
                "\n",
                "## For downloading the test data\n",
                "import os\n",
                "import requests\n",
                "import urllib\n",
                "import zipfile\n",
                "\n",
                "## For inserting test data\n",
                "import pandas as pd\n",
                "from sqlalchemy import create_engine\n",
                "\n",
                "## For using Intake\n",
                "from intake.catalog import Catalog\n",
                "\n",
                "## Global variables\n",
                "N_PARTITIONS = 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we download the data, if it doesn't already exist:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "\n",
                "# Define download sources and destinations.\n",
                "# For secure extraction, 'fpath' must be how the zip archive refers to the file.\n",
                "loan_data = {'url': 'https://resources.lendingclub.com/LoanStats3a.csv.zip',\n",
                "             'fpath': 'LoanStats3a.csv',\n",
                "             'table': 'loan_stats',\n",
                "             'date_col': 'issue_d',\n",
                "             'normalize': ['term', 'home_ownership', 'verification_status', 'loan_status', 'addr_state', 'application_type', 'disbursement_method']}\n",
                "decl_loan_data = {'url': 'https://resources.lendingclub.com/RejectStatsA.csv.zip',\n",
                "                  'fpath': 'RejectStatsA.csv',\n",
                "                  'table': 'reject_stats',\n",
                "                  'date_col': 'Application Date',\n",
                "                  'normalize': ['State']}\n",
                "\n",
                "# Do the data downloading and extraction\n",
                "for data in [loan_data, decl_loan_data]:\n",
                "    url, fpath = data['url'], data['fpath']\n",
                "    \n",
                "    if os.path.isfile(fpath):\n",
                "        print('{!r} already exists: skipping download.\\n'.format(fpath))\n",
                "        continue\n",
                "\n",
                "    try:\n",
                "        dl_fpath = os.path.basename(urllib.parse.urlsplit(url).path)\n",
                "        print('Downloading data from {!r}...'.format(url))\n",
                "        response = requests.get(url)\n",
                "    except:\n",
                "        raise ValueError('Download error. Check internet connection and URL.')\n",
                "\n",
                "    try:\n",
                "        with open(dl_fpath, 'wb') as fp:\n",
                "            print('Writing data...'.format(dl_fpath))\n",
                "            fp.write(response.content)\n",
                "\n",
                "        try:\n",
                "            print('Extracting data...')\n",
                "            with zipfile.ZipFile(dl_fpath, 'r') as zip_ref:\n",
                "                zip_ref.extract(fpath)\n",
                "            if os.path.isfile(dl_fpath) and dl_fpath.endswith('.zip'):\n",
                "                os.remove(dl_fpath)\n",
                "        except:\n",
                "            raise ValueError('File extraction error. Is the downloaded file a zip archive?')\n",
                "    except:\n",
                "        raise ValueError('File write error. Check destination file path and permissions')\n",
                "\n",
                "    print('Success: {!r}\\n'.format(fpath))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we partition the data into `N_PARTITIONS` groups, and persist each partition into a separate database instance. Although there are many ways we can choose to partition the dataset, here we partition by the date the loans were issued (or if they were rejected, the date when they were applied for):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": [
                "%time\n",
                "for data in [loan_data, decl_loan_data]:\n",
                "    fpath, date_col, table = data['fpath'], data['date_col'], data['table']\n",
                "    norm_cols = data['normalize']\n",
                "    pcol = '_' + date_col  # Used for partitioning the data\n",
                "    \n",
                "    df = pd.read_csv(fpath, skiprows=1)\n",
                "    print('# {}: {}'.format(table, len(df)))\n",
                "    print('# {} valued at N/A: {}'.format(table, len(df[df[date_col].isna()])))\n",
                "    df.dropna(axis=0, subset=[date_col], inplace=True)\n",
                "    \n",
                "    df[pcol] = pd.to_datetime(df[date_col]) # , format='%b-%Y')\n",
                "\n",
                "    # Cast strs with '%' into floats, so we can do analysis more easily\n",
                "    if 'int_rate' in df.columns:\n",
                "        df['int_rate'] = df['int_rate'].str.rstrip('%').astype(float)\n",
                "\n",
                "    df.sort_values(pcol, inplace=True)\n",
                "    grouped = df.groupby(pd.qcut(df[pcol],\n",
                "                                 N_PARTITIONS,\n",
                "                                 labels=list(range(N_PARTITIONS))))\n",
                "    \n",
                "    # Normalize what we can, store into first db instance\n",
                "    engine = create_engine('postgresql://postgres@localhost:{}/postgres'.format(5432))\n",
                "    for norm_col in norm_cols:\n",
                "        norm_col_cats = df[norm_col].astype('category')\n",
                "        norm_df = pd.DataFrame({'id': pd.np.arange(len(norm_col_cats.cat.categories)),\n",
                "                                norm_col: norm_col_cats.cat.categories.values})\n",
                "        df.loc[:, norm_col] = norm_col_cats.cat.codes\n",
                "        print('Persisting normalized column, {!r}...'.format(norm_col+'_codes'))\n",
                "        norm_df.to_sql(norm_col+'_codes', engine, if_exists='replace')\n",
                "    \n",
                "    for group_id, group_df in grouped:\n",
                "        print('\\n###', group_id)\n",
                "        start = group_df[pcol].min().strftime('%b-%Y')\n",
                "        end = group_df[pcol].max().strftime('%b-%Y')\n",
                "\n",
                "        # Save each partition to a different database\n",
                "        print('Persisting {} {} from {} to {}...'.format(len(group_df), table, start, end))\n",
                "        engine = create_engine('postgresql://postgres@localhost:{}/postgres'.format(5432+group_id))\n",
                "        try:\n",
                "            group_df.drop(columns=pcol).to_sql(table, engine, if_exists='fail') #'replace')\n",
                "        except ValueError:\n",
                "            pass  # Table already exists, so do nothing.\n",
                "        \n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Verify the data was written, by connecting to the databases directly with the `psql` command-line tool:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save each query from the `psql` command as HTML\n",
                "!for db_inst in $(seq 0 4); do \\\n",
                "    psql -h localhost -p $(expr 5432 + $db_inst) -U postgres -q -H \\\n",
                "        -c 'select loan_amnt, term, int_rate, issue_d from loan_stats limit 5;' \\\n",
                "      > db${db_inst}.html; \\\n",
                "done\n",
                "\n",
                "# Display the HTML files\n",
                "from IPython.display import display, HTML\n",
                "for db_inst in range(N_PARTITIONS):\n",
                "    display(HTML('db{}.html'.format(db_inst)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Reading the data (with Intake-Postgres)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Write out a __joins\\_catalog.yml__ file with the appropriate schema:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile joins_catalog.yml\n",
                "plugins:\n",
                "  source:\n",
                "    - module: intake_postgres\n",
                "\n",
                "sources:\n",
                "  # Normalized columns\n",
                "  term_codes:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: 'select id, term from term_codes'\n",
                "\n",
                "  home_ownership_codes:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: 'select id, home_ownership from home_ownership_codes'\n",
                "\n",
                "  verification_status_codes:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: 'select id, verification_status from verification_status_codes'\n",
                "\n",
                "  loan_status_codes:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: 'select id, loan_status from loan_status_codes'\n",
                "\n",
                "  addr_state_codes:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: 'select id, addr_state from addr_state_codes'\n",
                "\n",
                "  application_type_codes:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: 'select id, application_type from application_type_codes'\n",
                "\n",
                "  disbursement_method_codes:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: 'select id, disbursement_method from disbursement_method_codes'\n",
                "        \n",
                "  State_codes:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: 'select id, \"State\" from \"State_codes\"'\n",
                "\n",
                "\n",
                "  # loan_stats data\n",
                "  loans_1:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: 'select issue_d, term, application_type, disbursement_method, home_ownership, verification_status, loan_status, loan_amnt, int_rate from loan_stats'\n",
                "\n",
                "  loans_5:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5436/postgres'\n",
                "      sql_expr: 'select issue_d, term, application_type, disbursement_method, home_ownership, verification_status, loan_status, loan_amnt, int_rate from loan_stats'\n",
                "        \n",
                "\n",
                "  # reject_stats data\n",
                "  rejects_1:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: 'select \"Application Date\", \"State\", \"Amount Requested\" from reject_stats'\n",
                "\n",
                "  rejects_5:\n",
                "    driver: postgres\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5436/postgres'\n",
                "      sql_expr: 'select \"Application Date\", \"State\", \"Amount Requested\" from reject_stats'\n",
                "\n",
                "\n",
                "  # Joins\n",
                "  join_db_1_to_1:\n",
                "    driver: postgres\n",
                "    parameters:\n",
                "        - name: interest_lowbound\n",
                "          description: \"Lower-bound for interest rate in query\"\n",
                "          type: float\n",
                "          default: 0.0\n",
                "          min: 0.0\n",
                "    args:\n",
                "      uri: 'postgresql://postgres@localhost:5432/postgres'\n",
                "      sql_expr: !template \"\n",
                "        select issue_d,\n",
                "               term_codes.term,\n",
                "               application_type_codes.application_type,\n",
                "               disbursement_method_codes.disbursement_method,\n",
                "               home_ownership_codes.home_ownership,\n",
                "               verification_status_codes.verification_status,\n",
                "               loan_status_codes.loan_status,\n",
                "               loan_amnt,\n",
                "               int_rate\n",
                "        from loan_stats\n",
                "        inner join term_codes on loan_stats.term = term_codes.id\n",
                "        inner join application_type_codes on loan_stats.application_type = application_type_codes.id\n",
                "        inner join disbursement_method_codes on loan_stats.disbursement_method = disbursement_method_codes.id\n",
                "        inner join home_ownership_codes on loan_stats.home_ownership = home_ownership_codes.id\n",
                "        inner join verification_status_codes on loan_stats.verification_status = verification_status_codes.id\n",
                "        inner join loan_status_codes on loan_stats.loan_status = loan_status_codes.id\n",
                "        where int_rate > {{ interest_lowbound }}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Access the catalog with Intake:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%time\n",
                "catalog = Catalog('joins_catalog.yml')\n",
                "catalog"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Inspect the metadata about the first source (optional):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "catalog.loans_1.discover()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "catalog.application_type_codes.discover()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "catalog.join_db_1_to_1.discover()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Read the data from the sources:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "catalog.loans_1.read().tail()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "catalog.loans_5.read().tail()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## _JOIN_ with one database\n",
                "\n",
                "Here is our **JOIN**, with default parameters (`interest_lowbound == 0.0`):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "catalog.join_db_1_to_1.read().tail()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, with our own parameter value(s):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "catalog.join_db_1_to_1(interest_lowbound=15.0).read().tail()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## _JOIN_ with two databases\n",
                "\n",
                "For a **JOIN** between tables of two separate databases, we first connect to the tables we are interested in. Then we **JOIN** (aka `.merge()`) them together afterward:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "loans_5_df = catalog.loans_5.read()\n",
                "term_df = catalog.term_codes.read()\n",
                "application_type_df = catalog.application_type_codes.read()\n",
                "disbursement_method_df = catalog.disbursement_method_codes.read()\n",
                "home_ownership_df = catalog.home_ownership_codes.read()\n",
                "verification_status_df = catalog.verification_status_codes.read()\n",
                "loan_status_df = catalog.loan_status_codes.read()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "term_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "loans_5_df.tail()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for col, lookup_df in [('term', term_df),\n",
                "               ('application_type', application_type_df),\n",
                "               ('disbursement_method', disbursement_method_df),\n",
                "               ('home_ownership', home_ownership_df),\n",
                "               ('verification_status', verification_status_df),\n",
                "               ('loan_status', loan_status_df)]:\n",
                "    loans_5_df = pd.merge(loans_5_df, lookup_df,\n",
                "                          how='left', on=None,\n",
                "                          left_on=col, right_on='id',\n",
                "                          suffixes=['_', ''])\n",
                "    loans_5_df.drop(columns=col+'_', inplace=True)\n",
                "    if 'id_' in loans_5_df.columns:\n",
                "        loans_5_df.drop(columns='id_', inplace=True)\n",
                "    if 'id' in loans_5_df.columns:\n",
                "        loans_5_df.drop(columns='id', inplace=True)\n",
                "loans_5_df.tail()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
